{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC \n",
    "from joblib import load\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import color, filters\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import hough_circle, hough_circle_peaks\n",
    "from skimage.draw import circle_perimeter\n",
    "import urllib.request\n",
    "from skimage import measure\n",
    "import joblib\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x3, y3, x4, y4 = box2\n",
    "    intersection_area = max(0, min(x2, x4) - max(x1, x3)) * max(0, min(y2, y4) - max(y1, y3))\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x4 - x3) * (y4 - y3)\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    iou = intersection_area / union_area\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_precision_recall(gt_boxes, pred_boxes, confidence_scores):\n",
    "    # Calculate precision and recall for a single class\n",
    "    num_gt_boxes = len(gt_boxes)\n",
    "    num_pred_boxes = len(pred_boxes)\n",
    "\n",
    "    true_positives = np.zeros(num_pred_boxes)\n",
    "    false_positives = np.zeros(num_pred_boxes)\n",
    "    matched = [False for i in range(num_gt_boxes)]\n",
    "\n",
    "    for i in range(num_pred_boxes):\n",
    "        pred_box = pred_boxes[i]\n",
    "        max_iou = 0\n",
    "        max_iou_index = -1\n",
    "\n",
    "        gt_box = gt_boxes[i]\n",
    "        iou = calculate_iou(pred_box, gt_box)\n",
    "\n",
    "        if iou  >= 0.5:\n",
    "            if confidence_scores[i] > 0.5:  # Set confidence threshold here\n",
    "                if not matched[max_iou_index]:  # If the GT box is not matched yet\n",
    "                    true_positives[i] = 1\n",
    "                    matched[max_iou_index] = True  # Mark GT box as matched\n",
    "                else:\n",
    "                    false_positives[i] = 1\n",
    "        else:\n",
    "            false_positives[i] = 1\n",
    "\n",
    "    cum_true_positives = np.cumsum(true_positives)\n",
    "    cum_false_positives = np.cumsum(false_positives)\n",
    "    precision = cum_true_positives / (cum_true_positives + cum_false_positives)\n",
    "    recall = cum_true_positives / num_gt_boxes\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ap(precision, recall):\n",
    "    # Calculate Average Precision (AP) using precision and recall values\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    for i in range(len(mpre) - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "\n",
    "    return ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_traffic_light_detection(gt_annotations_list, pred_annotations_list):\n",
    "    # Evaluate traffic light detection using ground truth and predicted annotations\n",
    "\n",
    "    classes = ['red', 'yellow', 'green']\n",
    "    ap_values = []\n",
    "\n",
    "    for cls in classes:\n",
    "        gt_boxes_all_images = []\n",
    "        pred_boxes_all_images = []\n",
    "        confidence_scores_all_images = []\n",
    "\n",
    "        for i in range(len(gt_annotations_list)):\n",
    "            gt_annotations = gt_annotations_list[i]\n",
    "            pred_annotations = pred_annotations_list[i]\n",
    "\n",
    "            gt_boxes = np.array([box[1:5] for box in gt_annotations if box[5] == cls])\n",
    "            pred_boxes = np.array([box[1:5] for box in pred_annotations if box[5] == cls])\n",
    "            confidence_scores = np.array([box[6] for box in pred_annotations if box[5] == cls])\n",
    "\n",
    "            gt_matched = np.zeros(len(gt_boxes))\n",
    "\n",
    "            if len(gt_boxes) > 0:\n",
    "                for j in range(len(pred_boxes)):\n",
    "                    pred_box = pred_boxes[j]\n",
    "                    max_iou = 0\n",
    "                    max_iou_index = -1\n",
    "\n",
    "                    for k in range(len(gt_boxes)):\n",
    "                        gt_box = gt_boxes[k]\n",
    "                        iou = calculate_iou(pred_box, gt_box)\n",
    "\n",
    "                        if iou > max_iou:\n",
    "                            max_iou = iou\n",
    "                            max_iou_index = k\n",
    "\n",
    "                    if max_iou >= 0.5 and not gt_matched[max_iou_index]:\n",
    "                        gt_matched[max_iou_index] = 1\n",
    "\n",
    "                        gt_boxes_all_images.append(gt_boxes[max_iou_index])\n",
    "                        pred_boxes_all_images.append(pred_box)\n",
    "                        confidence_scores_all_images.append(confidence_scores[j])\n",
    "\n",
    "        gt_boxes_all_images = np.array(gt_boxes_all_images)\n",
    "        pred_boxes_all_images = np.array(pred_boxes_all_images)\n",
    "        confidence_scores_all_images = np.array(confidence_scores_all_images)\n",
    "        \n",
    "        precision, recall = calculate_precision_recall(gt_boxes_all_images, pred_boxes_all_images, confidence_scores_all_images)\n",
    "\n",
    "        ap = calculate_ap(precision, recall)\n",
    "\n",
    "        ap_values.append(ap)\n",
    "\n",
    "    mAP = np.mean(ap_values)\n",
    "    print(\"mAP@0.5: \", mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = ['go', 'stop', 'warning']\n",
    "color_map = {'go':'green', 'stop':'red', 'warning':'yellow'}\n",
    "\n",
    "train_folder_list = [\n",
    "    # 'dayTrain',\n",
    "    'daySequence1',\n",
    "    'daySequence2',\n",
    "#     'sample-dayClip6',\n",
    "    # 'nightTrain',\n",
    "    # 'nightSequence1',\n",
    "    # 'nightSequence2',\n",
    "#     'sample-nightClip1',\n",
    "]\n",
    "# test_percentage = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotarion_dataframe(train_data_folders):\n",
    "    data_base_path = '../input/lisa-traffic-light-dataset/'\n",
    "    annotation_list = list()\n",
    "    for folder in [folder + '/' for folder in train_data_folders if os.listdir(data_base_path)]:\n",
    "        annotation_path = ''\n",
    "        if 'sample' not in folder:\n",
    "            annotation_path = data_base_path + 'Annotations/Annotations/' + folder\n",
    "        else:\n",
    "            annotation_path = data_base_path + folder*2\n",
    "        image_frame_path = data_base_path + folder*2\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        if 'Clip' in os.listdir(annotation_path)[0]:\n",
    "            clip_list = os.listdir(annotation_path)\n",
    "            for clip_folder in clip_list:\n",
    "                df = pd.read_csv(annotation_path + clip_folder +  '/frameAnnotationsBOX.csv', sep=\";\")\n",
    "                df['image_path'] = image_frame_path + clip_folder + '/frames/'\n",
    "                annotation_list.append(df)\n",
    "        else:\n",
    "            df = pd.read_csv(annotation_path +  'frameAnnotationsBOX.csv', sep=\";\")\n",
    "            df['image_path'] = image_frame_path + 'frames/'\n",
    "            annotation_list.append(df)\n",
    "        \n",
    "    df = pd.concat(annotation_list)\n",
    "    df = df.drop(['Origin file', 'Origin frame number', 'Origin track', 'Origin track frame number'], axis=1)\n",
    "    df.columns = ['filename', 'target', 'x1', 'y1', 'x2', 'y2', 'image_path']\n",
    "    df = df[df['target'].isin(target_classes)]\n",
    "    df['filename'] = df['filename'].apply(lambda filename: re.findall(\"\\/([\\d\\w-]*.jpg)\", filename)[0])\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daySequence1--00000.jpg</td>\n",
       "      <td>stop</td>\n",
       "      <td>706</td>\n",
       "      <td>478</td>\n",
       "      <td>718</td>\n",
       "      <td>500</td>\n",
       "      <td>../input/lisa-traffic-light-dataset/daySequenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daySequence1--00001.jpg</td>\n",
       "      <td>stop</td>\n",
       "      <td>705</td>\n",
       "      <td>475</td>\n",
       "      <td>720</td>\n",
       "      <td>497</td>\n",
       "      <td>../input/lisa-traffic-light-dataset/daySequenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daySequence1--00002.jpg</td>\n",
       "      <td>stop</td>\n",
       "      <td>707</td>\n",
       "      <td>476</td>\n",
       "      <td>719</td>\n",
       "      <td>494</td>\n",
       "      <td>../input/lisa-traffic-light-dataset/daySequenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>daySequence1--00005.jpg</td>\n",
       "      <td>stop</td>\n",
       "      <td>708</td>\n",
       "      <td>474</td>\n",
       "      <td>720</td>\n",
       "      <td>492</td>\n",
       "      <td>../input/lisa-traffic-light-dataset/daySequenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>daySequence1--00006.jpg</td>\n",
       "      <td>stop</td>\n",
       "      <td>707</td>\n",
       "      <td>470</td>\n",
       "      <td>722</td>\n",
       "      <td>492</td>\n",
       "      <td>../input/lisa-traffic-light-dataset/daySequenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14320</th>\n",
       "      <td>daySequence2--06881.jpg</td>\n",
       "      <td>go</td>\n",
       "      <td>388</td>\n",
       "      <td>0</td>\n",
       "      <td>448</td>\n",
       "      <td>50</td>\n",
       "      <td>../input/lisa-traffic-light-dataset/daySequenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14321</th>\n",
       "      <td>daySequence2--06881.jpg</td>\n",
       "      <td>go</td>\n",
       "      <td>1099</td>\n",
       "      <td>218</td>\n",
       "      <td>1144</td>\n",
       "      <td>283</td>\n",
       "      <td>../input/lisa-traffic-light-dataset/daySequenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14322</th>\n",
       "      <td>daySequence2--06882.jpg</td>\n",
       "      <td>go</td>\n",
       "      <td>1138</td>\n",
       "      <td>198</td>\n",
       "      <td>1183</td>\n",
       "      <td>273</td>\n",
       "      <td>../input/lisa-traffic-light-dataset/daySequenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14323</th>\n",
       "      <td>daySequence2--06883.jpg</td>\n",
       "      <td>go</td>\n",
       "      <td>1184</td>\n",
       "      <td>171</td>\n",
       "      <td>1232</td>\n",
       "      <td>256</td>\n",
       "      <td>../input/lisa-traffic-light-dataset/daySequenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14324</th>\n",
       "      <td>daySequence2--06884.jpg</td>\n",
       "      <td>go</td>\n",
       "      <td>1232</td>\n",
       "      <td>134</td>\n",
       "      <td>1280</td>\n",
       "      <td>244</td>\n",
       "      <td>../input/lisa-traffic-light-dataset/daySequenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14325 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filename target    x1   y1    x2   y2  \\\n",
       "0      daySequence1--00000.jpg   stop   706  478   718  500   \n",
       "1      daySequence1--00001.jpg   stop   705  475   720  497   \n",
       "2      daySequence1--00002.jpg   stop   707  476   719  494   \n",
       "3      daySequence1--00005.jpg   stop   708  474   720  492   \n",
       "4      daySequence1--00006.jpg   stop   707  470   722  492   \n",
       "...                        ...    ...   ...  ...   ...  ...   \n",
       "14320  daySequence2--06881.jpg     go   388    0   448   50   \n",
       "14321  daySequence2--06881.jpg     go  1099  218  1144  283   \n",
       "14322  daySequence2--06882.jpg     go  1138  198  1183  273   \n",
       "14323  daySequence2--06883.jpg     go  1184  171  1232  256   \n",
       "14324  daySequence2--06884.jpg     go  1232  134  1280  244   \n",
       "\n",
       "                                              image_path  \n",
       "0      ../input/lisa-traffic-light-dataset/daySequenc...  \n",
       "1      ../input/lisa-traffic-light-dataset/daySequenc...  \n",
       "2      ../input/lisa-traffic-light-dataset/daySequenc...  \n",
       "3      ../input/lisa-traffic-light-dataset/daySequenc...  \n",
       "4      ../input/lisa-traffic-light-dataset/daySequenc...  \n",
       "...                                                  ...  \n",
       "14320  ../input/lisa-traffic-light-dataset/daySequenc...  \n",
       "14321  ../input/lisa-traffic-light-dataset/daySequenc...  \n",
       "14322  ../input/lisa-traffic-light-dataset/daySequenc...  \n",
       "14323  ../input/lisa-traffic-light-dataset/daySequenc...  \n",
       "14324  ../input/lisa-traffic-light-dataset/daySequenc...  \n",
       "\n",
       "[14325 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotation_df = get_annotarion_dataframe(train_folder_list)\n",
    "\n",
    "target_classes = train_annotation_df['target'].unique()\n",
    "target_classes.sort()\n",
    "\n",
    "train_annotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, x1, y1, x2, y2):\n",
    "    cropped_image = image[y1:y2, x1:x2]\n",
    "    cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bouding_boxes(path):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Convert the image to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    # Convert the image array to L*a*b* color space\n",
    "    lab_image = color.rgb2lab(image_array)\n",
    "\n",
    "    # Extract the L, a, and b channels\n",
    "    L = lab_image[:, :, 0]\n",
    "    a = lab_image[:, :, 1]\n",
    "    b = lab_image[:, :, 2]\n",
    "\n",
    "\n",
    "    # Calculate the RGYB channel\n",
    "    RGYB = 0.7*L * (1.0*a + 0.7*b)\n",
    "    YELLOW = 0.7*L * (0.7*a + b)\n",
    "\n",
    "\n",
    "    # Create a mask for red and green blobs\n",
    "    red_mask = np.logical_and(RGYB > 2000, L <= 80)\n",
    "    green_mask = np.logical_and(RGYB < -2000, L <= 80)\n",
    "    yellow_mask = np.logical_and(YELLOW > 1000, RGYB <= 2000)\n",
    "\n",
    "    # Apply the masks to the original image\n",
    "    red_blobs = np.zeros_like(image_array)\n",
    "    red_blobs[red_mask] = image_array[red_mask]\n",
    "    green_blobs = np.zeros_like(image_array)\n",
    "    green_blobs[green_mask] = image_array[green_mask]\n",
    "    yellow_blobs = np.zeros_like(image_array)\n",
    "    yellow_blobs[yellow_mask] = image_array[yellow_mask]\n",
    "\n",
    "    # Convert the red and green blobs to grayscale\n",
    "    red_gray = color.rgb2gray(red_blobs)\n",
    "    green_gray = color.rgb2gray(green_blobs)\n",
    "    yellow_gray = color.rgb2gray(yellow_blobs)\n",
    "\n",
    "    red_gray = np.where(red_gray > 0, 255, 0).astype(np.uint8)\n",
    "    green_gray = np.where(green_gray > 0, 255, 0).astype(np.uint8)\n",
    "    yellow_gray = np.where(yellow_gray > 0, 255, 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "    # Use built-in sobel filter in hough circle transform.\n",
    "    red_edges = red_gray\n",
    "    green_edges = green_gray\n",
    "    yellow_edges = yellow_gray\n",
    "\n",
    "    # Perform Hough circle detection on red edges\n",
    "    red_radii = np.arange(5, 30)  # Define the expected radius range for red circles\n",
    "    red_hough = hough_circle(red_edges, red_radii)\n",
    "    red_accums, red_centers_x, red_centers_y, red_radii = hough_circle_peaks(red_hough, red_radii, threshold=0.90*np.max(red_hough), min_xdistance = 50, min_ydistance = 50)\n",
    "\n",
    "    # Perform Hough circle detection on green edges\n",
    "    green_radii = np.arange(5, 30)  # Define the expected radius range for red circles\n",
    "    green_hough = hough_circle(green_edges, green_radii)\n",
    "    green_accums, green_centers_x, green_centers_y, green_radii = hough_circle_peaks(green_hough, green_radii, threshold=0.90*np.max(green_hough), min_xdistance=50, min_ydistance = 50)\n",
    "\n",
    "    # Perform Hough circle detection on yellow edges\n",
    "    yellow_radii = np.arange(5, 30)  # Define the expected radius range for yellow circles\n",
    "    yellow_hough = hough_circle(yellow_edges, yellow_radii)\n",
    "    yellow_accums, yellow_centers_x, yellow_centers_y, yellow_radii = hough_circle_peaks(yellow_hough, yellow_radii, threshold=0.90*np.max(yellow_hough), min_xdistance=50, min_ydistance = 50)\n",
    "\n",
    "\n",
    "    # Draw detected circles on the original image\n",
    "\n",
    "    cropped_images = []\n",
    "    bounding_boxes = []\n",
    "    labels = []\n",
    "\n",
    "    red_circles_image = np.copy(image_array)\n",
    "    for center_y, center_x, radius in zip(red_centers_y, red_centers_x, red_radii):\n",
    "        circy, circx = circle_perimeter(center_y, center_x, radius)\n",
    "        valid_coords = np.logical_and(circy >= 0, circy < red_circles_image.shape[0]) & np.logical_and(circx >= 0, circx < red_circles_image.shape[1])\n",
    "        red_circles_image[circy[valid_coords], circx[valid_coords]] = (0, 255, 0)  # Red color for circles\n",
    "        max_x = red_circles_image.shape[1]\n",
    "        max_y = red_circles_image.shape[0]\n",
    "        x1, y1, x2, y2 = max(0,center_x - 1.9*radius), max(0, center_y - 1.9*radius), min(max_x, center_x + 1.9*radius), min(max_y, center_y + (7.0)*radius)\n",
    "        x1, x2, y1, y2 = round(x1), round(x2), round(y1), round(y2) \n",
    "        cropped_images.append(crop_image(image_array, x1, y1, x2, y2))\n",
    "        bounding_boxes.append([x1, y1, x2, y2])\n",
    "        labels.append(\"red\")\n",
    "\n",
    "\n",
    "    green_circles_image = np.copy(image_array)\n",
    "    for center_y, center_x, radius in zip(green_centers_y, green_centers_x, green_radii):\n",
    "        circy, circx = circle_perimeter(center_y, center_x, radius)\n",
    "        valid_coords = np.logical_and(circy >= 0, circy < green_circles_image.shape[0]) & np.logical_and(circx >= 0, circx < green_circles_image.shape[1])\n",
    "        green_circles_image[circy[valid_coords], circx[valid_coords]] = (255, 0, 0)  # Green color for circles\n",
    "        max_x = green_circles_image.shape[1]\n",
    "        max_y = green_circles_image.shape[0]\n",
    "        x1, y1, x2, y2 = max(0,center_x - 1.9*radius), max(0, center_y - 7.0*radius), min(max_x, center_x + 1.9*radius), min(max_y, center_y + (1.9)*radius)\n",
    "        x1, x2, y1, y2 = round(x1), round(x2), round(y1), round(y2) \n",
    "        cropped_images.append(crop_image(image_array, x1, y1, x2, y2))\n",
    "        bounding_boxes.append([x1, y1, x2, y2])\n",
    "        labels.append(\"green\")\n",
    "        \n",
    "    yellow_circles_image = np.copy(image_array)\n",
    "    for center_y, center_x, radius in zip(yellow_centers_y, yellow_centers_x, yellow_radii):\n",
    "        circy, circx = circle_perimeter(center_y, center_x, radius)\n",
    "        valid_coords = np.logical_and(circy >= 0, circy < yellow_circles_image.shape[0]) & np.logical_and(circx >= 0, circx < yellow_circles_image.shape[1])\n",
    "        yellow_circles_image[circy[valid_coords], circx[valid_coords]] = (0, 0, 255)  # yellow color for circles\n",
    "        max_x = yellow_circles_image.shape[1]\n",
    "        max_y = yellow_circles_image.shape[0]\n",
    "        x1, y1, x2, y2 = max(0,center_x - 1.9*radius), max(0, center_y - 4.1*radius), min(max_x, center_x + 1.0*radius), min(max_y, center_y + (4.1)*radius)\n",
    "        x1, x2, y1, y2 = round(x1), round(x2), round(y1), round(y2) \n",
    "        cropped_images.append(crop_image(image_array, x1, y1, x2, y2))\n",
    "        bounding_boxes.append([x1, y1, x2, y2])\n",
    "        labels.append(\"yellow\")\n",
    "    \n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, x1, y1, x2, y2):\n",
    "    # cropped_image = image.crop((x1, y1, x2, y2))\n",
    "    cropped_image = image[y1:y2, x1:x2, :]\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['yellow']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['yellow']\n",
      "(2040,)\n",
      "['yellow']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['green']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['green']\n",
      "(2040,)\n",
      "['green']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['green']\n",
      "(2040,)\n",
      "['green']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['yellow']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['yellow']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['yellow']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['green']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['green']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['green']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['green']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['green']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['green']\n",
      "(2040,)\n",
      "['green']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['yellow']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['yellow']\n",
      "(2040,)\n",
      "['red']\n",
      "(2040,)\n",
      "['not_traffic_light']\n",
      "(2040,)\n",
      "['not_traffic_light']\n"
     ]
    }
   ],
   "source": [
    "unique_filenames = train_annotation_df['filename'].unique()\n",
    "id_1 = 0\n",
    "id_2 = 0\n",
    "\n",
    "gt_annotations_list = []\n",
    "pred_annotations_list = []\n",
    "clf = joblib.load(\"clf.joblib\")\n",
    "test_percentage = 0.004\n",
    "idx = 0\n",
    "\n",
    "\n",
    "# Iterate over each unique filename\n",
    "for filename in (unique_filenames):\n",
    "    # Filter rows with the current filename\n",
    "    random_number = np.random.rand()\n",
    "    if (random_number > test_percentage):\n",
    "        continue\n",
    "\n",
    "    filtered_rows = train_annotation_df[train_annotation_df['filename'] == filename]\n",
    "    img_path = filtered_rows.iloc[0]['image_path']\n",
    "    img_name = filtered_rows.iloc[0]['filename']\n",
    "    img = Image.open(os.path.join(img_path, img_name))\n",
    "\n",
    "    current_gt_anotations = []\n",
    "    current_pred_anotations = []\n",
    "    X_test = []\n",
    "\n",
    "\n",
    "    predicted_bounding_boxes = generate_bouding_boxes(img_path + img_name)\n",
    "    for x1, y1, x2, y2 in predicted_bounding_boxes:\n",
    "        cropped_image = img.crop((x1, y1, x2, y2))\n",
    "        cropped_image = cropped_image.resize((20,34))\n",
    "        cropped_image = np.array(cropped_image).flatten()  # Flatten the image\n",
    "        cropped_image = cropped_image.astype(np.float64)/255\n",
    "        current_pred_anotations.append([img_path+img_name, x1, y1, x2, y2])\n",
    "        print(cropped_image.shape)\n",
    "        predd = clf.predict([cropped_image])\n",
    "        print(predd)\n",
    "        X_test.append(cropped_image)\n",
    "\n",
    "    if (len(X_test) == 0):\n",
    "        continue\n",
    "    idx+= 1\n",
    "    y_pred = clf.predict(X_test).tolist()\n",
    "    y_pred_prob = np.max(clf.predict_proba(X_test), axis = 1).tolist()\n",
    "    concatenated_list = [a + [b, c] for a, b, c in zip((current_pred_anotations), (y_pred), (y_pred_prob))]\n",
    "\n",
    "    pred_annotations_list.append(concatenated_list)\n",
    "\n",
    "\n",
    "    for index, row in filtered_rows.iterrows():\n",
    "        x1 = row['x1']\n",
    "        y1 = row['y1']\n",
    "        x2 = row['x2']\n",
    "        y2 = row['y2']\n",
    "        target = row['target']\n",
    "        target = color_map[target]\n",
    "        current_gt_anotations.append([img_path+img_name, x1, y1, x2, y2, target])\n",
    "        # gt_annotations_list.append([x1, y1, x2, y2, target])\n",
    "    gt_annotations_list.append(current_gt_anotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00133.jpg', 756, 425, 774, 452, 'red']]\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00133.jpg', 562, 516, 580, 560, 'not_traffic_light', 0.8403403695128541], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00133.jpg', 494, 516, 514, 560, 'not_traffic_light', 0.7458701538261593], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00133.jpg', 432, 424, 447, 464, 'not_traffic_light', 0.8005564172351962], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00133.jpg', 494, 504, 509, 544, 'yellow', 0.5103810285273925]]\n",
      "---\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00759.jpg', 610, 381, 628, 408, 'red'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00759.jpg', 743, 434, 761, 461, 'red'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00759.jpg', 383, 430, 401, 461, 'red']]\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00759.jpg', 202, 704, 222, 748, 'red', 0.9843503723525537], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00759.jpg', 762, 705, 788, 767, 'not_traffic_light', 0.9805806270148557], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00759.jpg', 222, 694, 236, 734, 'red', 0.7504336430722491], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00759.jpg', 510, 548, 524, 590, 'not_traffic_light', 0.7839775747863066], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00759.jpg', 753, 694, 770, 744, 'not_traffic_light', 0.5903996753976627]]\n",
      "---\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00850.jpg', 743, 433, 761, 460, 'red'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00850.jpg', 608, 381, 626, 408, 'red'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00850.jpg', 383, 430, 401, 457, 'red']]\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00850.jpg', 204, 702, 222, 747, 'red', 0.96888635615032], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00850.jpg', 762, 705, 788, 767, 'not_traffic_light', 0.9668351775800813], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00850.jpg', 224, 696, 239, 736, 'not_traffic_light', 0.8574753720911006], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00850.jpg', 749, 698, 766, 748, 'not_traffic_light', 0.8915578689236722]]\n",
      "---\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00920.jpg', 608, 381, 626, 408, 'red'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00920.jpg', 743, 435, 761, 462, 'red'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00920.jpg', 384, 431, 402, 458, 'red']]\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00920.jpg', 202, 695, 224, 748, 'not_traffic_light', 0.9659007430915606], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00920.jpg', 761, 708, 783, 761, 'not_traffic_light', 0.8113240546996188], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00920.jpg', 222, 691, 239, 741, 'not_traffic_light', 0.825459774251887], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00920.jpg', 508, 548, 523, 590, 'yellow', 0.5849979146738374], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--00920.jpg', 738, 696, 752, 738, 'yellow', 0.7622321863024604]]\n",
      "---\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01238.jpg', 794, 388, 812, 418, 'green'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01238.jpg', 600, 313, 618, 343, 'green'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01238.jpg', 269, 375, 290, 410, 'green']]\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01238.jpg', 440, 556, 458, 601, 'red', 0.5955951610011314], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01238.jpg', 673, 557, 695, 610, 'not_traffic_light', 0.6594011797663226], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01238.jpg', 598, 307, 616, 352, 'red', 0.5524793618089032], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01238.jpg', 662, 548, 676, 590, 'not_traffic_light', 0.9157423272563262]]\n",
      "---\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01243.jpg', 796, 401, 814, 431, 'green'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01243.jpg', 597, 324, 615, 354, 'green'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01243.jpg', 260, 389, 278, 429, 'green']]\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01243.jpg', 674, 580, 694, 624, 'not_traffic_light', 0.8055432526836963], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01243.jpg', 418, 576, 438, 621, 'red', 0.5049326196114898], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01243.jpg', 658, 570, 672, 610, 'not_traffic_light', 0.8964818241505865]]\n",
      "---\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01952.jpg', 597, 262, 630, 311, 'red'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01952.jpg', 762, 269, 792, 314, 'red'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01952.jpg', 1052, 379, 1085, 424, 'red']]\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01952.jpg', 1058, 380, 1078, 424, 'red', 0.8273941873519777], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01952.jpg', 766, 268, 786, 313, 'red', 0.5687705445650525], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01952.jpg', 1058, 368, 1072, 408, 'not_traffic_light', 0.9728380857902471], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01952.jpg', 768, 258, 782, 298, 'not_traffic_light', 0.999699265967534], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--01952.jpg', 604, 248, 618, 290, 'not_traffic_light', 0.9999364581141945]]\n",
      "---\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02560.jpg', 751, 455, 763, 475, 'green'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02560.jpg', 545, 129, 593, 209, 'green'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02560.jpg', 800, 150, 842, 220, 'green']]\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02560.jpg', 582, 444, 602, 488, 'not_traffic_light', 0.859543800294529], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02560.jpg', 814, 175, 834, 220, 'green', 0.9956453507862864], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02560.jpg', 1116, 550, 1130, 592, 'not_traffic_light', 0.7783935492183592]]\n",
      "---\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02755.jpg', 835, 434, 853, 459, 'green'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02755.jpg', 708, 379, 720, 409, 'green'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02755.jpg', 638, 378, 653, 403, 'green']]\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02755.jpg', 636, 368, 656, 412, 'not_traffic_light', 0.9942433672667434], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02755.jpg', 206, 906, 223, 956, 'not_traffic_light', 0.9932129130686129], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02755.jpg', 378, 734, 393, 774, 'not_traffic_light', 0.9979960979531514], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02755.jpg', 255, 847, 275, 905, 'not_traffic_light', 0.9975545148195981]]\n",
      "---\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02823.jpg', 766, 234, 796, 284, 'green'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02823.jpg', 1089, 361, 1119, 411, 'green'], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02823.jpg', 597, 216, 630, 271, 'green']]\n",
      "[['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02823.jpg', 604, 227, 622, 272, 'green', 0.9519422128216025], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02823.jpg', 770, 238, 790, 282, 'green', 0.9952337254202739], ['../input/lisa-traffic-light-dataset/daySequence1/daySequence1/frames/daySequence1--02823.jpg', 1092, 474, 1107, 514, 'not_traffic_light', 0.5998115631122315]]\n",
      "F1 score: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def evaluate(gt_annotations_list, pred_annotations_list):\n",
    "    # Evaluate traffic light detection using ground truth and predicted annotations\n",
    "\n",
    "\n",
    "gt_boxes_all_images = []\n",
    "pred_boxes_all_images = []\n",
    "confidence_scores_all_images = []\n",
    "gt_classes_all_images = []\n",
    "pred_classes_all_images = []\n",
    "\n",
    "for i in range(len(gt_annotations_list)):\n",
    "    gt_annotations = gt_annotations_list[i]\n",
    "    pred_annotations = pred_annotations_list[i]\n",
    "\n",
    "    gt_boxes = np.array([box[1:5] for box in gt_annotations])\n",
    "    pred_boxes = np.array([box[1:5] for box in pred_annotations])\n",
    "    confidence_scores = np.array([box[6] for box in pred_annotations])\n",
    "    pred_classes = np.array([box[5] for box in pred_annotations])\n",
    "    gt_classes = np.array([box[5] for box in gt_annotations])\n",
    "\n",
    "    gt_matched = np.zeros(len(gt_boxes))\n",
    "\n",
    "    if len(gt_boxes) > 0:\n",
    "        for j in range(len(pred_boxes)):\n",
    "            pred_box = pred_boxes[j]\n",
    "            pred_class = pred_classes[j]\n",
    "            max_iou = 0\n",
    "            max_iou_index = -1\n",
    "\n",
    "            for k in range(len(gt_boxes)):\n",
    "                gt_box = gt_boxes[k]\n",
    "                iou = calculate_iou(pred_box, gt_box)\n",
    "\n",
    "                if iou > max_iou:\n",
    "                    max_iou = iou\n",
    "                    max_iou_index = k\n",
    "\n",
    "            if max_iou >= 0.5 and not gt_matched[max_iou_index]:\n",
    "                gt_matched[max_iou_index] = 1\n",
    "\n",
    "                gt_boxes_all_images.append(gt_boxes[max_iou_index])\n",
    "                pred_boxes_all_images.append(pred_box)\n",
    "                confidence_scores_all_images.append(confidence_scores[j])\n",
    "                gt_boxes_all_images.append(gt_boxes[max_iou_index])\n",
    "                gt_classes_all_images.append(gt_classes[max_iou_index])\n",
    "                pred_classes_all_images.append(pred_class)\n",
    "\n",
    "gt_boxes_all_images = np.array(gt_boxes_all_images)\n",
    "pred_boxes_all_images = np.array(pred_boxes_all_images)\n",
    "\n",
    "# Calculate F1 score\n",
    "for i in range(10):\n",
    "    print(\"---\")\n",
    "    print(gt_annotations_list[i])\n",
    "    print(pred_annotations_list[i])\n",
    "f1 = f1_score(pred_classes_all_images, gt_classes_all_images, average='weighted')\n",
    "print('F1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@0.5:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "evaluate_traffic_light_detection(gt_annotations_list, pred_annotations_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
